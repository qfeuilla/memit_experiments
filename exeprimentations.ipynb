{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from easy_transformer import EasyTransformer\n",
    "from easy_transformer.hook_points import HookPoint\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is used in order to ensure that there is no overlapping in target memory to edit\n",
    "@dataclass\n",
    "class Memory:\n",
    "    subject : str\n",
    "    relation : str\n",
    "    objectif : str\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.s = self.subject\n",
    "        self.r = self.relation\n",
    "        self.o = self.objectif\n",
    "\n",
    "@dataclass\n",
    "class Request:\n",
    "    # holding the Subject, Relation and Object to target\n",
    "    mem : Memory\n",
    "    prompt : str\n",
    "    examples : List[str] = field(default_factory=list)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RequestList:\n",
    "    __requests : List[Request] = field(default_factory=list)\n",
    "\n",
    "    def append(self, requests : List[Request]):\n",
    "        # if requests not already a list, change it to one\n",
    "        if type(requests) != list:\n",
    "            requests = [requests]\n",
    "\n",
    "        for request in requests:\n",
    "            #region check for conflicting requets\n",
    "            \n",
    "            # Check if there is no conflicting (subject, relation)\n",
    "            # if there is one, just skip this request\n",
    "            skip = False\n",
    "            for i, saved_request in enumerate(self.__requests): \n",
    "                \n",
    "                # get the memory elements from saved request and current request\n",
    "                saved_s, saved_r, *_ = saved_request.mem.__dict__.values()\n",
    "                s, r, *_ = request.mem.__dict__.values()\n",
    "\n",
    "                if saved_s == s and saved_r == r:\n",
    "                    print(f\"skipping ({s},{r}) because it is in conflict with ({saved_s},{saved_r}) at index {i}\")\n",
    "                    skip = True\n",
    "                    continue\n",
    "            if skip:\n",
    "                continue\n",
    "            \n",
    "            #endregion\n",
    "            self.__requests.append(request)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = RequestList()\n",
    "requests.append([\n",
    "    Request(\n",
    "        mem=Memory(\"Lebron James\", \"play sport\", \"Football\"), \n",
    "        prompt=\"{} is playing the sport of\", \n",
    "        examples=[\n",
    "            \"Lebron James was famousely known for\"\n",
    "        ]\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be one of\n",
    "# ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'facebook/opt-125m', 'facebook/opt-1.3b', 'facebook/opt-2.7b', 'facebook/opt-6.7b', 'facebook/opt-13b', 'facebook/opt-30b', 'facebook/opt-66b', 'EleutherAI/gpt-neo-125M', 'EleutherAI/gpt-neo-1.3B', 'EleutherAI/gpt-neo-2.7B', 'EleutherAI/gpt-j-6B', 'EleutherAI/gpt-neox-20b']\n",
    "MODEL_NAME = \"gpt2-xl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n",
      "Finished loading pretrained model gpt2-xl into EasyTransformer!\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = EasyTransformer.from_pretrained(MODEL_NAME)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untuple(x):\n",
    "    return x[0] if isinstance(x, tuple) else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layername(num=0, _type=None):\n",
    "    # Layer translation for top level hooks on EasyTranformer\n",
    "    supported_hooks = [\"embed\", \"pos_embed\", \"resid_pre\", \"resid_mid\", \"resid_post\", \"attn_out\", \"mlp_out\"]\n",
    "    assert _type in supported_hooks, f\"type must be one of {supported_hooks}\"\n",
    "    \n",
    "    if _type == \"embed\":\n",
    "        return \"hook_embed\"\n",
    "    \n",
    "    if _type == \"pos_embed\":\n",
    "        return \"hook_pos_embed\"\n",
    "\n",
    "    return f\"blocks.{num}.hook_{_type}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_processed_inputs(model : EasyTransformer, _inputs : List[str], hooks = None):\n",
    "    # Forward\n",
    "    toks = [model.tokenizer.tokenize(i) for i in _inputs]\n",
    "\n",
    "    out = model(_inputs)\n",
    "\n",
    "    # Logits to probs\n",
    "    probs = torch.stack([torch.softmax(out[i, len(tok)], 0) for i, tok in enumerate(toks)])\n",
    "    print(probs.shape)\n",
    "\n",
    "    # get best value infos\n",
    "    p, preds = torch.max(probs, dim=-1)\n",
    "\n",
    "    return preds, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_predict(model : EasyTransformer, prompts : List[str]):\n",
    "    if type(prompts) != list:\n",
    "        prompts = [prompts]\n",
    "\n",
    "    # get predictions + probabilities\n",
    "    preds, ps = predict_processed_inputs(model, prompts)\n",
    "    \n",
    "    # decode resutls\n",
    "    result = [model.tokenizer.decode(c) for c in preds]\n",
    "    \n",
    "    return [{\n",
    "        \"prediction\" : r,\n",
    "        \"prob\" : p\n",
    "    } for r, p in zip(result, ps.detach().cpu().numpy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50257])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prediction': ' Paris', 'prob': 0.7615234},\n",
       " {'prediction': ' Seattle', 'prob': 0.96716756}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_predict(model, [\"The Eiffel Tower is in the city of\", \"The Space Needle is in the city of\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_datasets = [\n",
    "    (\"The Space Needle is in the city of\", \" Seatle\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Patches():\n",
    "    # Range of tokens to corrupt\n",
    "    tokens_to_mix : Tuple[int, int] = None\n",
    "    # list of (index to patch, layer to patch at)\n",
    "    states_to_patch : List[Tuple[int, str]] = field(default_factory=list)\n",
    "    # How much noise to add\n",
    "    noise : float = 0.1\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.rs = np.random.RandomState(42)\n",
    "        \n",
    "        self.device = device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        # reform patches for easier process inside the hook\n",
    "        self.patches = defaultdict(list)\n",
    "        for t, l in self.states_to_patch:\n",
    "            self.patches[l].append(t)\n",
    "\n",
    "    def embed_hook(self, embed : torch.Tensor, hook : HookPoint):\n",
    "        # embed of shape (batch_size, n_tokens, embed_size)\n",
    "        bs, _, es = embed.shape\n",
    "\n",
    "        if self.tokens_to_mix is not None:\n",
    "            begin, end = self.tokens_to_mix\n",
    "\n",
    "            # Corrupt every element of the batch except the first one (for later uncorruption)\n",
    "            embed[1:, begin:end] += self.noise * torch.from_numpy(\n",
    "                self.rs.randn(bs - 1, end - begin, es)\n",
    "            ).to(self.device)\n",
    "\n",
    "        return embed\n",
    "\n",
    "    def layer_hook(self, x : torch.Tensor, hook : HookPoint):\n",
    "        print(hook.name, \"is being patched\")\n",
    "        \n",
    "        # Loop through tokens on which to restore the right data\n",
    "        for t in self.patches[hook.name]:\n",
    "            x[1:, t] = x[0, t]\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def filter_hook_to_patch(self, name):\n",
    "        return name in self.patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_with_patch(\n",
    "        model : EasyTransformer,\n",
    "        inputs : List[str],\n",
    "        answer_token : int,\n",
    "        patches : Patches\n",
    "    ):\n",
    "    processed_hooks = [\n",
    "        (layername(_type=\"embed\"), patches.embed_hook),\n",
    "        (patches.filter_hook_to_patch, patches.layer_hook)\n",
    "    ]\n",
    "\n",
    "    outputs = model.run_with_hooks(inputs, fwd_hooks=processed_hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.2.hook_attn_out is being patched\n"
     ]
    }
   ],
   "source": [
    "forward_with_patch(\n",
    "    model,\n",
    "    prompt_datasets[0][0],\n",
    "    10,\n",
    "    Patches(\n",
    "        (0, 4),\n",
    "        [(2, layername(2, \"attn_out\"))]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f286f93fee40caea4c0bb0162ba9fef2c5ef2273dbbdbb9b0c6ff0ce519c43aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
